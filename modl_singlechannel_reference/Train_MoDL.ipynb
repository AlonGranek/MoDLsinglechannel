{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import os, sys\n",
    "import logging\n",
    "import random\n",
    "import h5py\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sigpy.plot as pl\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "# import custom libraries\n",
    "from utils import transforms as T\n",
    "from utils import subsample as ss\n",
    "from utils import complex_utils as cplx\n",
    "from utils.resnet2p1d import generate_model\n",
    "from utils.flare_utils import roll\n",
    "# import custom classes\n",
    "from utils.datasets import SliceData\n",
    "from subsample_fastmri import MaskFunc\n",
    "from MoDL_single import UnrolledModel\n",
    "import argparse\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "%load_ext autoreload\n",
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training unrolled reconstruction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, args, use_seed=False):\n",
    "        self.mask_func = mask_func\n",
    "        self.use_seed = use_seed\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, kspace, target, reference, reference_kspace,slice):\n",
    "        im_lowres = abs(sp.ifft(sp.resize(sp.resize(kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals = im_lowres.reshape(-1)\n",
    "        k = int(round(0.05 * magnitude_vals.shape[0]))\n",
    "        scale = magnitude_vals[magnitude_vals.argsort()[::-1][k]]\n",
    "        kspace = kspace/scale\n",
    "        target = target/scale\n",
    "        # Convert everything from numpy arrays to tensors\n",
    "        kspace_torch = cplx.to_tensor(kspace).float()   \n",
    "        target_torch = cplx.to_tensor(target).float()   \n",
    "        mask_slice = np.ones((256,160))\n",
    "        mk1 = self.mask_func((1,1,160,2))[0,0,:,0]\n",
    "        knee_masks = mask_slice*mk1\n",
    "        mask_torch = torch.tensor(knee_masks[...,None]).float()\n",
    "        # Use poisson mask instead\n",
    "        mask2 = sp.mri.poisson((256,160), 2, calib=(32, 22), dtype=float, crop_corner=True, return_density=False, seed=0, max_attempts=6, tol=0.1)\n",
    "        mask_torch = torch.stack([torch.tensor(mask2).float(),torch.tensor(mask2).float()],dim=2)\n",
    "        kspace_torch = kspace_torch*mask_torch\n",
    "\n",
    "        ### Reference addition ###\n",
    "        im_lowres_ref = abs(sp.ifft(sp.resize(sp.resize(reference_kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals_ref = im_lowres_ref.reshape(-1)\n",
    "        k_ref = int(round(0.05 * magnitude_vals_ref.shape[0]))\n",
    "        scale_ref = magnitude_vals_ref[magnitude_vals_ref.argsort()[::-1][k_ref]]\n",
    "        reference = reference / scale_ref\n",
    "        reference_torch = cplx.to_tensor(reference).float()\n",
    "\n",
    "        return kspace_torch,target_torch,mask_torch, reference_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # Generate k-t undersampling masks\n",
    "    train_mask = MaskFunc([0.08],[4])\n",
    "    train_data = SliceData(\n",
    "        root=str(args.data_path),\n",
    "        transform=DataTransform(train_mask, args),\n",
    "        sample_rate=1\n",
    "    )\n",
    "    return train_data\n",
    "def create_data_loaders(args):\n",
    "    train_data = create_datasets(args)\n",
    "#     print(train_data[0])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader\n",
    "def build_optim(args, params):\n",
    "    optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "params = Namespace()\n",
    "#params.data_path = \"../../single_channel_data/train/\"\n",
    "params.data_path = \"./registered_data/\"\n",
    "params.batch_size = 4\n",
    "params.num_grad_steps = 4\n",
    "params.num_cg_steps = 8\n",
    "params.share_weights = True\n",
    "params.modl_lamda = 0.05\n",
    "params.lr = 0.00001\n",
    "params.weight_decay = 0\n",
    "params.lr_step_size = 500\n",
    "params.lr_gamma = 0.5\n",
    "params.epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tal/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loaders(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights\n"
     ]
    }
   ],
   "source": [
    "single_MoDL = UnrolledModel(params).to(device)\n",
    "optimizer = build_optim(params, single_MoDL.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, params.lr_step_size, params.lr_gamma)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch = [  0/ 21] Iter = [   0/ 154] Loss = 0.002451 Avg Loss = 0.002451\n",
      "INFO:root:Epoch = [  0/ 21] Iter = [ 125/ 154] Loss = 0.001862 Avg Loss = 0.00206\n",
      "INFO:root:Epoch = [  1/ 21] Iter = [   0/ 154] Loss = 0.001327 Avg Loss = 0.001327\n",
      "INFO:root:Epoch = [  1/ 21] Iter = [ 125/ 154] Loss = 0.001209 Avg Loss = 0.001219\n",
      "INFO:root:Epoch = [  2/ 21] Iter = [   0/ 154] Loss = 0.001069 Avg Loss = 0.001069\n",
      "INFO:root:Epoch = [  2/ 21] Iter = [ 125/ 154] Loss = 0.0007657 Avg Loss = 0.0009348\n",
      "INFO:root:Epoch = [  3/ 21] Iter = [   0/ 154] Loss = 0.0008442 Avg Loss = 0.0008442\n",
      "INFO:root:Epoch = [  3/ 21] Iter = [ 125/ 154] Loss = 0.0006008 Avg Loss = 0.0007737\n",
      "INFO:root:Epoch = [  4/ 21] Iter = [   0/ 154] Loss = 0.0006524 Avg Loss = 0.0006524\n",
      "INFO:root:Epoch = [  4/ 21] Iter = [ 125/ 154] Loss = 0.0007083 Avg Loss = 0.0006623\n",
      "INFO:root:Epoch = [  5/ 21] Iter = [   0/ 154] Loss = 0.0005935 Avg Loss = 0.0005935\n",
      "INFO:root:Epoch = [  5/ 21] Iter = [ 125/ 154] Loss = 0.0005462 Avg Loss = 0.0006034\n",
      "INFO:root:Epoch = [  6/ 21] Iter = [   0/ 154] Loss = 0.0005846 Avg Loss = 0.0005846\n",
      "INFO:root:Epoch = [  6/ 21] Iter = [ 125/ 154] Loss = 0.0005586 Avg Loss = 0.0005653\n",
      "INFO:root:Epoch = [  7/ 21] Iter = [   0/ 154] Loss = 0.0004694 Avg Loss = 0.0004694\n",
      "INFO:root:Epoch = [  7/ 21] Iter = [ 125/ 154] Loss = 0.0005959 Avg Loss = 0.0005009\n",
      "INFO:root:Epoch = [  8/ 21] Iter = [   0/ 154] Loss = 0.0005777 Avg Loss = 0.0005777\n",
      "INFO:root:Epoch = [  8/ 21] Iter = [ 125/ 154] Loss = 0.0005401 Avg Loss = 0.0005111\n",
      "INFO:root:Epoch = [  9/ 21] Iter = [   0/ 154] Loss = 0.00048 Avg Loss = 0.00048\n",
      "INFO:root:Epoch = [  9/ 21] Iter = [ 125/ 154] Loss = 0.0005172 Avg Loss = 0.0004617\n",
      "INFO:root:Epoch = [ 10/ 21] Iter = [   0/ 154] Loss = 0.0003909 Avg Loss = 0.0003909\n",
      "INFO:root:Epoch = [ 10/ 21] Iter = [ 125/ 154] Loss = 0.0003907 Avg Loss = 0.0004224\n",
      "INFO:root:Epoch = [ 11/ 21] Iter = [   0/ 154] Loss = 0.0004327 Avg Loss = 0.0004327\n",
      "INFO:root:Epoch = [ 11/ 21] Iter = [ 125/ 154] Loss = 0.0004955 Avg Loss = 0.0004135\n",
      "INFO:root:Epoch = [ 12/ 21] Iter = [   0/ 154] Loss = 0.0003779 Avg Loss = 0.0003779\n",
      "INFO:root:Epoch = [ 12/ 21] Iter = [ 125/ 154] Loss = 0.0003151 Avg Loss = 0.0004053\n",
      "INFO:root:Epoch = [ 13/ 21] Iter = [   0/ 154] Loss = 0.0003454 Avg Loss = 0.0003454\n",
      "INFO:root:Epoch = [ 13/ 21] Iter = [ 125/ 154] Loss = 0.000367 Avg Loss = 0.0003648\n",
      "INFO:root:Epoch = [ 14/ 21] Iter = [   0/ 154] Loss = 0.000372 Avg Loss = 0.000372\n",
      "INFO:root:Epoch = [ 14/ 21] Iter = [ 125/ 154] Loss = 0.0003771 Avg Loss = 0.0003623\n",
      "INFO:root:Epoch = [ 15/ 21] Iter = [   0/ 154] Loss = 0.0003721 Avg Loss = 0.0003721\n",
      "INFO:root:Epoch = [ 15/ 21] Iter = [ 125/ 154] Loss = 0.000315 Avg Loss = 0.0003512\n",
      "INFO:root:Epoch = [ 16/ 21] Iter = [   0/ 154] Loss = 0.000349 Avg Loss = 0.000349\n",
      "INFO:root:Epoch = [ 16/ 21] Iter = [ 125/ 154] Loss = 0.0003431 Avg Loss = 0.0003383\n",
      "INFO:root:Epoch = [ 17/ 21] Iter = [   0/ 154] Loss = 0.0003064 Avg Loss = 0.0003064\n",
      "INFO:root:Epoch = [ 17/ 21] Iter = [ 125/ 154] Loss = 0.000333 Avg Loss = 0.000319\n",
      "INFO:root:Epoch = [ 18/ 21] Iter = [   0/ 154] Loss = 0.0002955 Avg Loss = 0.0002955\n",
      "INFO:root:Epoch = [ 18/ 21] Iter = [ 125/ 154] Loss = 0.000272 Avg Loss = 0.0003079\n",
      "INFO:root:Epoch = [ 19/ 21] Iter = [   0/ 154] Loss = 0.0003891 Avg Loss = 0.0003891\n",
      "INFO:root:Epoch = [ 19/ 21] Iter = [ 125/ 154] Loss = 0.0003076 Avg Loss = 0.0003291\n",
      "INFO:root:Epoch = [ 20/ 21] Iter = [   0/ 154] Loss = 0.0002762 Avg Loss = 0.0002762\n",
      "INFO:root:Epoch = [ 20/ 21] Iter = [ 125/ 154] Loss = 0.0003501 Avg Loss = 0.0002895\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(params.epoch):\n",
    "    single_MoDL.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        input,target,mask,reference = data\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        mask = mask.to(device)\n",
    "        reference = reference.to(device)\n",
    "\n",
    "        im_out = single_MoDL(input.float(),reference_image=reference,mask=mask)\n",
    "        loss = criterion(im_out,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        if iter % 125 == 0:\n",
    "            logging.info(\n",
    "                f'Epoch = [{epoch:3d}/{params.epoch:3d}] '\n",
    "                f'Iter = [{iter:4d}/{len(train_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g}'\n",
    "            )\n",
    "    #Saving the model\n",
    "    exp_dir = \"L2_checkpoints_poisson_x4_test/\"\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'params': params,\n",
    "                'model': single_MoDL.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'exp_dir': exp_dir\n",
    "            },\n",
    "            f=os.path.join(exp_dir, 'model_%d.pt'%(epoch))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
