{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os, sys\n",
    "import logging\n",
    "import random\n",
    "import h5py\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sigpy.plot as pl\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "# import custom libraries\n",
    "from utils import transforms as T\n",
    "from utils import subsample as ss\n",
    "from utils import complex_utils as cplx\n",
    "from utils.resnet2p1d import generate_model\n",
    "from utils.flare_utils import roll\n",
    "# import custom classes\n",
    "from utils.datasets import SliceData\n",
    "from subsample_fastmri import MaskFunc\n",
    "from MoDL_single import UnrolledModel\n",
    "import argparse\n",
    "from models.SAmodel import MyNetwork\n",
    "from models.Unrolled import Unrolled\n",
    "from models.UnrolledRef import UnrolledRef\n",
    "from models.UnrolledTransformer import UnrolledTrans\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "%load_ext autoreload\n",
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training unrolled reconstruction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, args, use_seed=False):\n",
    "        self.mask_func = mask_func\n",
    "        self.use_seed = use_seed\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, kspace, target, reference, reference_kspace,slice):\n",
    "        im_lowres = abs(sp.ifft(sp.resize(sp.resize(kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals = im_lowres.reshape(-1)\n",
    "        k = int(round(0.05 * magnitude_vals.shape[0]))\n",
    "        scale = magnitude_vals[magnitude_vals.argsort()[::-1][k]]\n",
    "        kspace = kspace/scale\n",
    "        target = target/scale\n",
    "        # Convert everything from numpy arrays to tensors\n",
    "        kspace_torch = cplx.to_tensor(kspace).float()   \n",
    "        target_torch = cplx.to_tensor(target).float()   \n",
    "        # Use poisson mask instead\n",
    "        mask2 = sp.mri.poisson((256,160), 5, calib=(18, 14), dtype=float, crop_corner=False, return_density=True, seed=0, max_attempts=6, tol=0.01)\n",
    "        mask_torch = torch.stack([torch.tensor(mask2).float(),torch.tensor(mask2).float()],dim=2)\n",
    "        mask_torch = T.kspace_crop(mask_torch,0.67)\n",
    "        #kspace_torch = T.kspace_cut(mask_torch,0.5)\n",
    "        kspace_torch = T.awgn_torch(kspace_torch,15,L=1)\n",
    "        kspace_torch = kspace_torch*mask_torch\n",
    "\n",
    "        ### Reference addition ###\n",
    "        im_lowres_ref = abs(sp.ifft(sp.resize(sp.resize(reference_kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals_ref = im_lowres_ref.reshape(-1)\n",
    "        k_ref = int(round(0.05 * magnitude_vals_ref.shape[0]))\n",
    "        scale_ref = magnitude_vals_ref[magnitude_vals_ref.argsort()[::-1][k_ref]]\n",
    "        reference = reference / scale_ref\n",
    "        reference_torch = cplx.to_tensor(reference).float()\n",
    "\n",
    "        return kspace_torch,target_torch,mask_torch, reference_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # Generate k-t undersampling masks\n",
    "    train_mask = MaskFunc([0.08],[4])\n",
    "    train_data = SliceData(\n",
    "        root=str(args.data_path),\n",
    "        transform=DataTransform(train_mask, args),\n",
    "        sample_rate=1\n",
    "    )\n",
    "    return train_data\n",
    "def create_data_loaders(args):\n",
    "    train_data = create_datasets(args)\n",
    "#     print(train_data[0])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader\n",
    "def build_optim(args, params):\n",
    "    optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "params = Namespace()\n",
    "#params.data_path = \"./registered_data/patient23b/\"\n",
    "params.data_path = \"./registered_data/\"\n",
    "params.batch_size = 4\n",
    "params.num_grad_steps = 4 #4\n",
    "params.num_cg_steps = 8\n",
    "params.share_weights = True\n",
    "params.modl_lamda = 0.05\n",
    "params.lr = 0.00001\n",
    "#params.lr = 0.0001\n",
    "params.weight_decay = 0\n",
    "params.lr_step_size = 10\n",
    "params.lr_gamma = 0.5\n",
    "params.epoch = 21\n",
    "params.reference_mode = 0\n",
    "params.reference_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tal/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loaders(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tal/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "#single_MoDL = UnrolledModel(params).to(device)\n",
    "#single_MoDL = MyNetwork(2,2).to(device)\n",
    "#single_MoDL = Unrolled(params).to(device)\n",
    "single_MoDL = UnrolledRef(params).to(device)\n",
    "#single_MoDL = UnrolledTrans(params).to(device)\n",
    "optimizer = build_optim(params, single_MoDL.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, params.lr_step_size, params.lr_gamma)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch = [  0/ 21] Iter = [   0/ 149] Loss = 14.42 Avg Loss = 14.42\n",
      "INFO:root:Epoch = [  0/ 21] Iter = [ 125/ 149] Loss = 0.1281 Avg Loss = 4.333\n",
      "INFO:root:Epoch = [  1/ 21] Iter = [   0/ 149] Loss = 0.09137 Avg Loss = 0.09137\n",
      "INFO:root:Epoch = [  1/ 21] Iter = [ 125/ 149] Loss = 0.045 Avg Loss = 0.07482\n",
      "INFO:root:Epoch = [  2/ 21] Iter = [   0/ 149] Loss = 0.06797 Avg Loss = 0.06797\n",
      "INFO:root:Epoch = [  2/ 21] Iter = [ 125/ 149] Loss = 0.03423 Avg Loss = 0.04686\n",
      "INFO:root:Epoch = [  3/ 21] Iter = [   0/ 149] Loss = 0.0248 Avg Loss = 0.0248\n",
      "INFO:root:Epoch = [  3/ 21] Iter = [ 125/ 149] Loss = 0.02397 Avg Loss = 0.0254\n",
      "INFO:root:Epoch = [  4/ 21] Iter = [   0/ 149] Loss = 0.02189 Avg Loss = 0.02189\n",
      "INFO:root:Epoch = [  4/ 21] Iter = [ 125/ 149] Loss = 0.01636 Avg Loss = 0.02015\n",
      "INFO:root:Epoch = [  5/ 21] Iter = [   0/ 149] Loss = 0.01616 Avg Loss = 0.01616\n",
      "INFO:root:Epoch = [  5/ 21] Iter = [ 125/ 149] Loss = 0.01509 Avg Loss = 0.01559\n",
      "INFO:root:Epoch = [  6/ 21] Iter = [   0/ 149] Loss = 0.01625 Avg Loss = 0.01625\n",
      "INFO:root:Epoch = [  6/ 21] Iter = [ 125/ 149] Loss = 0.01365 Avg Loss = 0.01376\n",
      "INFO:root:Epoch = [  7/ 21] Iter = [   0/ 149] Loss = 0.01264 Avg Loss = 0.01264\n",
      "INFO:root:Epoch = [  7/ 21] Iter = [ 125/ 149] Loss = 0.009752 Avg Loss = 0.01148\n",
      "INFO:root:Epoch = [  8/ 21] Iter = [   0/ 149] Loss = 0.009416 Avg Loss = 0.009416\n",
      "INFO:root:Epoch = [  8/ 21] Iter = [ 125/ 149] Loss = 0.01052 Avg Loss = 0.009616\n",
      "INFO:root:Epoch = [  9/ 21] Iter = [   0/ 149] Loss = 0.009223 Avg Loss = 0.009223\n",
      "INFO:root:Epoch = [  9/ 21] Iter = [ 125/ 149] Loss = 0.008634 Avg Loss = 0.008879\n",
      "INFO:root:Epoch = [ 10/ 21] Iter = [   0/ 149] Loss = 0.008128 Avg Loss = 0.008128\n",
      "INFO:root:Epoch = [ 10/ 21] Iter = [ 125/ 149] Loss = 0.007205 Avg Loss = 0.007883\n",
      "INFO:root:Epoch = [ 11/ 21] Iter = [   0/ 149] Loss = 0.007247 Avg Loss = 0.007247\n",
      "INFO:root:Epoch = [ 11/ 21] Iter = [ 125/ 149] Loss = 0.007262 Avg Loss = 0.007141\n",
      "INFO:root:Epoch = [ 12/ 21] Iter = [   0/ 149] Loss = 0.006638 Avg Loss = 0.006638\n",
      "INFO:root:Epoch = [ 12/ 21] Iter = [ 125/ 149] Loss = 0.005832 Avg Loss = 0.00651\n",
      "INFO:root:Epoch = [ 13/ 21] Iter = [   0/ 149] Loss = 0.00675 Avg Loss = 0.00675\n",
      "INFO:root:Epoch = [ 13/ 21] Iter = [ 125/ 149] Loss = 0.007319 Avg Loss = 0.006171\n",
      "INFO:root:Epoch = [ 14/ 21] Iter = [   0/ 149] Loss = 0.007858 Avg Loss = 0.007858\n",
      "INFO:root:Epoch = [ 14/ 21] Iter = [ 125/ 149] Loss = 0.005424 Avg Loss = 0.006168\n",
      "INFO:root:Epoch = [ 15/ 21] Iter = [   0/ 149] Loss = 0.00488 Avg Loss = 0.00488\n",
      "INFO:root:Epoch = [ 15/ 21] Iter = [ 125/ 149] Loss = 0.005162 Avg Loss = 0.005043\n",
      "INFO:root:Epoch = [ 16/ 21] Iter = [   0/ 149] Loss = 0.004908 Avg Loss = 0.004908\n",
      "INFO:root:Epoch = [ 16/ 21] Iter = [ 125/ 149] Loss = 0.004193 Avg Loss = 0.004772\n",
      "INFO:root:Epoch = [ 17/ 21] Iter = [   0/ 149] Loss = 0.004542 Avg Loss = 0.004542\n",
      "INFO:root:Epoch = [ 17/ 21] Iter = [ 125/ 149] Loss = 0.004552 Avg Loss = 0.004473\n",
      "INFO:root:Epoch = [ 18/ 21] Iter = [   0/ 149] Loss = 0.004558 Avg Loss = 0.004558\n",
      "INFO:root:Epoch = [ 18/ 21] Iter = [ 125/ 149] Loss = 0.004592 Avg Loss = 0.004322\n",
      "INFO:root:Epoch = [ 19/ 21] Iter = [   0/ 149] Loss = 0.003456 Avg Loss = 0.003456\n",
      "INFO:root:Epoch = [ 19/ 21] Iter = [ 125/ 149] Loss = 0.003758 Avg Loss = 0.003845\n",
      "INFO:root:Epoch = [ 20/ 21] Iter = [   0/ 149] Loss = 0.003753 Avg Loss = 0.003753\n",
      "INFO:root:Epoch = [ 20/ 21] Iter = [ 125/ 149] Loss = 0.004017 Avg Loss = 0.003696\n"
     ]
    }
   ],
   "source": [
    "### Load for fine-tunning\n",
    "checkpoint_file = \"./L2_checkpoints_poisson_x4_SAunrolledRefOF/model_20.pt\"\n",
    "checkpoint = torch.load(checkpoint_file,map_location=device)\n",
    "#params = checkpoint[\"params\"]\n",
    "single_MoDL.load_state_dict(checkpoint['model'])\n",
    "\n",
    "for epoch in range(params.epoch):\n",
    "    single_MoDL.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        input,target,mask,reference = data\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        mask = mask.to(device)\n",
    "        reference = reference.to(device)\n",
    "\n",
    "        im_out = single_MoDL(input.float(),reference_image=reference,mask=mask)\n",
    "        loss = criterion(im_out,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        if iter % 125 == 0:\n",
    "            logging.info(\n",
    "                f'Epoch = [{epoch:3d}/{params.epoch:3d}] '\n",
    "                f'Iter = [{iter:4d}/{len(train_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g}'\n",
    "            )\n",
    "    #Saving the model\n",
    "    exp_dir = \"L2_checkpoints_poisson_x4_SAunrolledRefOF/\"\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'params': params,\n",
    "                'model': single_MoDL.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'exp_dir': exp_dir\n",
    "            },\n",
    "            f=os.path.join(exp_dir, 'model_%d.pt'%(epoch))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
